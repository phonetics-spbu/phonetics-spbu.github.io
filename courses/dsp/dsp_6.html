<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Введение в машинное обучение: линейная регрессия</title>
    <style>
        .chapter-content {
            font-family: Arial, sans-serif;
            line-height: 1.6;
        }

        h2 {
            color: #2c3e50;
        }

        .note {
            background: #f8f9fa;
            padding: 10px;
            border-left: 3px solid #3498db;
        }
    </style>
</head>
<body>
<p><i>Компьютерная программа считается обучающейся на опыте E относительно некоторого класса задач T и меры качества P,
    если её качество на задачах из T, измеренное с помощью P, улучшается с накоплением опыта E. </i>(Том Митчелл,
    1997г.)</p>
<p>Различают машинное обучение с учителем (элементы выборки размечены) и без учителя (данные не содержат разметки,
    модель сама учится решать задачу).
    Обучение без учителя включает в себя задачи классификации (предсказание типа нового объекта) и регрессии
    (предсказание численной характеристики). Примером
    обучения без учителя являются модели, решающие задачу кластеризации (выделения групп похожих объектов).</p>

<p>Основные этапы:</p>
<ol>
    <li>Получение данных</li>
    <li>Исследовательский анализ данных (выявление шумов, скрытых закономерностей)</li>
    <li>Подготовка факторов (характеристик объекта), отбор нужных. Векторизация.</li>
    <li>Создание модели машинного обучения, подбор параметров</li>
    <li>Оценка качества на тестовой выборке</li>
</ol>

<p>Выборка делится на обучающую, тестовую и валидационную. Модель содержит параметры (подбираются на обучающей выборке) и гиперпараметры
    (фиксируются до этапа настройки параметров). Подбор оптимальных гиперпараметров можно осуществлять с помощью grid search c заданным шагом.</p>
<p>Примеры моделей:</p>
<p><b>Классификация:</b> наивная байесовская классификация, логистическая регрессия, дерево принятия решений, случайный лес, градиентный бустинг, нейронные сети и т.д.</p>
<p><b>Регрессия:</b> линейная регрессия, полиномиальная регрессия, случайный лес, градиентный бустинг, нейронные сети и т.д.</p>
<p><b>Кластеризация:</b> метод k средних, иерархическая кластеризация, DBSCAN</p>
<h3>Линейная регрессия</h3>
<p>$$\hat{y} = \omega_0 + \omega_1 x_1 + ... + \omega_n x_n$$
Такая модель хорошоо интерпретируема - чем больше по модулю коэффициент &omega; перед фактором x, тем этот фактор важнее. Хорошо работает данная модель в том случае, если
    целевая характеристика линейно зависит от фактора.
</p>
<p>Пусть модель обучается на n объектах, а число факторов равно m.
    $$ X =
    \begin{pmatrix}
    x_{11} & x_{12} & ... & x_{1m} \\
    x_{21} & x_{22} & ... & x_{2m} \\
    \dots \\
    x_{n1} & x_{n2} & ... & x_{nm}
    \end{pmatrix}
    $$
</p>
<p>Добавим столбец с единичками для обучения свободного коэффициента и зададим вектор весов модели &omega;:
    $$ X =
    \begin{pmatrix}
    1 & x_{11} & x_{12} & ... & x_{1m} \\
    1 & x_{21} & x_{22} & ... & x_{2m} \\
    \dots \\
    1 & x_{n1} & x_{n2} & ... & x_{nm}
    \end{pmatrix} ;

    \omega =
    \begin{pmatrix}
    \omega_{0} \\
    \omega_{1} \\
    \dots \\
    \omega_{m}
    \end{pmatrix}
    $$
</p>
<p> Задача сводится к нахождению &omega;, обеспечивающих минимизацию разницы между предсказанием и реальными данными (функции потерь):
$$
\Vert y - \hat{y} \Vert = \Vert y - X\omega \Vert
$$
</p>

<p> Решение этой задачи:
$$
\omega = (X^{T} X)^{-1} X^{T}y
$$
</p>

<p> Основные функции потерь:
    </br>
    </br>
    MAE (mean absolute error)
$$
loss(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y_i}|
$$

        MSE (mean squared error)
$$
loss(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2
$$
</p>


<p class="note">
    <b>Колаб: линейная регрессия</b>
    <br/>
    <a
            target="_blank"
            href="https://colab.research.google.com/github/phonetics-spbu/phonetics-spbu.github.io/blob/main/public/courses/dsp/ipynb/ML_linear_regression.ipynb"
    >
        Открыть в Colab
    </a>
    <br/>

    <a
            target="_blank"
            href="https://github.com/phonetics-spbu/phonetics-spbu.github.io/blob/main/public/courses/dsp/ipynb/ML_linear_regression.ipynb"
    >
        Открыть на Github
    </a>
    <br/>
    <a
            href="courses/dsp/ipynb/ML_linear_regression.ipynb"
            download
    >
        Скачать на устройство
    </a>
</p>


</body>
</html>